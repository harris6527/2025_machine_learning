---
title: Week 3 Assignment

---


## âœï¸ Written assignment â€” è§£é‡‹ Lemma 3.1 èˆ‡ 3.2

### 0) èƒŒæ™¯èˆ‡ç›´è¦º
æŠŠ $\tanh$ è¦–ç‚ºä¸€å¡Šã€Œå¯å¹³ç§»ã€å¯ç¸®æ”¾ã€çš„ç©æœ¨ï¼š$\sigma(wx+b)=\tanh(wx+b)$ã€‚  
è‹¥å°ç¸®æ”¾åƒæ•¸ $w$ ä½œå¾®åˆ†ï¼ˆæˆ–ä»¥å·®åˆ†è¿‘ä¼¼å¾®åˆ†ï¼‰ï¼ŒæœƒæŠŠè¼¸å…¥çš„å†ªæ¬¡å¸¶å‡ºä¾†ï¼š
$$
\frac{d^p}{dw^p}\sigma(wx+b) = x^p\,\sigma^{(p)}(wx+b).
$$
åœ¨ $w=0$ è©•ä¼°ä¸¦åšé©ç•¶æ¨™æº–åŒ–ï¼ˆæˆ–ç”¨é«˜éšä¸­å¿ƒå·®åˆ†è¿‘ä¼¼ï¼‰ï¼Œå³å¯å¾—åˆ°ã€Œ**åƒ $x^p$ çš„ç©æœ¨**ã€ã€‚é€™æ˜¯ã€Œç”¨ $\tanh$ çµ„å‡ºå¤šé …å¼ã€çš„æ ¸å¿ƒç›´è¦ºã€‚

---

### 1) Lemma 3.1ï¼ˆå¥‡æ¬¡å†ªçš„åŒæ™‚è¿‘ä¼¼ï¼‰
**æƒ³æ³•ï¼ˆç™½è©±ï¼‰**  
åœ¨ä¸€å€‹å°å€é–“ $[-\delta,\delta]$ å…§ï¼Œå¸Œæœ›åŒæ™‚è¿‘ä¼¼å¤šå€‹å¥‡æ¬¡å†ª $x, x^3, \dots, x^s$ï¼ˆ$s$ ç‚ºå¥‡æ•¸ï¼‰ï¼Œè€Œä¸”é€£åŒå…¶å°æ•¸ï¼ˆåœ¨ $W^{k,\infty}$ çš„æ„ç¾©ï¼‰ã€‚  
ä½œæ³•æ˜¯ç”¨**ä¸­å¿ƒå·®åˆ†**çµ„å‡ºã€Œå¤©ç„¶ç‚ºå¥‡å‡½æ•¸ã€çš„ç©æœ¨ï¼š
$$
\frac{\sigma(hx+b)-\sigma(-hx+b)}{2h} \approx C_1\,x \quad(\text{å° } h \text{ æ™‚}),
$$
æ›æˆé«˜éšä¸­å¿ƒå·®åˆ†ï¼Œèƒ½å¾—åˆ°ã€Œåƒ $x^3$ã€ã€ã€Œåƒ $x^5$ã€ç­‰çš„ç©æœ¨ã€‚æŠŠé€™äº›ç©æœ¨ç·šæ€§çµ„åˆï¼Œå°±å¯ä»¥**åŒæ™‚**è¿‘ä¼¼ $x, x^3, \dots, x^s$ï¼Œä¸”ç”±æ–¼ $\tanh$ å¹³æ»‘ï¼Œ**å°æ•¸èª¤å·®ä¹Ÿèƒ½æ§åˆ¶**ã€‚

**é‡é»çµè«–ï¼ˆç›´è§€ç‰ˆï¼‰**
- å­˜åœ¨å–®éš±è—å±¤ç¶²è·¯ï¼Œ**å¯¬åº¦ç´„èˆ‡ $s$ æˆæ­£æ¯”**ï¼Œå¯åœ¨ $[-\delta,\delta]$ å…§æŠŠæ‰€æœ‰å¥‡å†ªï¼ˆåˆ° $s$ï¼‰åŒæ™‚è¿‘ä¼¼åˆ°çµ¦å®šç²¾åº¦ã€‚  
- è¿‘ä¼¼å¯å»¶ä¼¸åˆ°**å°æ•¸å±¤ç´š**ï¼ˆ$W^{k,\infty}$ èª¤å·®ç•Œï¼‰ã€‚  
- æ¬Šé‡å¤§å°èˆ‡ $\varepsilon,\delta$ æœ‰å®šé‡é—œä¿‚ï¼ˆå€é–“è¶Šå°ã€å®¹è¨±èª¤å·®è¶Šå¤§è¶Šå®¹æ˜“ï¼‰ã€‚

---

### 2) Lemma 3.2ï¼ˆæŠŠå¶å†ªä¹Ÿæ‹‰é€²ä¾†ï¼‰
**æƒ³æ³•ï¼ˆç™½è©±ï¼‰**  
æ—¢ç„¶å¥‡å†ªéƒ½èƒ½è¿‘ä¼¼ï¼Œé‚£å¦‚ä½•è™•ç†å¶å†ª $x^2,x^4,\dots$ï¼Ÿ  
é—œéµåœ¨**ä»£æ•¸æ†ç­‰å¼**ï¼Œä¾‹å¦‚
$$
(y+\alpha)^{2n+1}-(y-\alpha)^{2n+1} = \sum_{j=0}^n c_j(\alpha)\,y^{2j+1},
$$
å¯æŠŠé«˜å¥‡å†ªå±•æˆ**å¥‡å†ªçš„ç·šæ€§çµ„åˆ**ã€‚å†ç¶“ç”±éè¿´æ¶ˆå»è¼ƒä½å¶å†ªï¼Œå°‡ $y^{2n}$ å¯«æˆ**å¥‡å†ªçš„ç·šæ€§çµ„åˆ**ã€‚å› ç‚ºå¥‡å†ªå·²å¯ç”¨ $\tanh$ è¿‘ä¼¼ï¼Œå¶å†ªå°±è·Ÿè‘—è¢«è¿‘ä¼¼åˆ°äº†ã€‚

**é‡é»çµè«–ï¼ˆç›´è§€ç‰ˆï¼‰**
- é€éç´”ä»£æ•¸æ‹†è§£ï¼Œå°‡**å¶å†ª $\to$ å¥‡å†ªçš„ç·šæ€§çµ„åˆ**ã€‚  
- çµåˆ Lemma 3.1 çš„å¥‡å†ªç©æœ¨åº«ï¼Œé”æˆã€Œ**å¥‡ï¼‹å¶**ã€åˆ°ä»»æ„ $p\le s$ çš„**å…¨éƒ¨å†ªæ¬¡**è¿‘ä¼¼ï¼ˆå«å°æ•¸ï¼‰ã€‚  
- å¯¬åº¦ä»ç„¶åƒ…**ç·šæ€§ä¾è³´æ–¼ $s$**ï¼Œå› ç‚ºåªæ˜¯é‡ç”¨å¥‡å†ªç©æœ¨ã€‚

---

### 3) ç–‘å•
**æˆ‘å­¸åˆ°çš„**
- $\tanh$ çš„**ä¸­å¿ƒå·®åˆ†**å¤©ç„¶çµ¦å¥‡å‡½æ•¸ç©æœ¨ï¼ˆè¿‘ä¼¼ $x, x^3, \dots$ï¼‰ï¼Œæ˜¯å»ºæ§‹å¤šé …å¼è¿‘ä¼¼çš„ç›´è§€å·¥å…·ã€‚  
- ä»¥ä»£æ•¸æ†ç­‰å¼æŠŠå¶å†ªè¡¨ç¤ºç‚ºå¥‡å†ªçµ„åˆï¼Œæ–¼æ˜¯åªè¦å¥‡å†ªå¯è¿‘ä¼¼ï¼Œå¶å†ªä¹Ÿèƒ½è¾¦åˆ°ã€‚  
- çµæœæ˜¯**å¯å»ºæ§‹**ä¸”**å«å°æ•¸**çš„èª¤å·®ä¿è­‰ï¼ˆ$W^{k,\infty}$ï¼‰ã€‚

**ä»æƒ³é‡æ¸…**
- æ¬Šé‡å°ºåº¦å°æ•¸å€¼ç©©å®šæ€§çš„å¯¦ä½œå½±éŸ¿ï¼ˆæ˜¯å¦é€ æˆæ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±ï¼‰ã€‚  
- å€é–“åŠ å¤§ï¼ˆå¦‚ $[-1,1]$ï¼‰æ˜¯å¦éœ€å¤šå±¤æˆ–åˆ†æ®µæ‹¼æ¥ä¾†ç¶­æŒç›¸ä¼¼èª¤å·®ç•Œã€‚  
- è‹¥æ´»åŒ–å‡½æ•¸æ›æˆéå…‰æ»‘ï¼ˆå¦‚ ReLUï¼‰ï¼Œä¸­å¿ƒå·®åˆ†/é«˜éšå°æ•¸çš„è¿‘ä¼¼çµè«–å¦‚ä½•ä¿®æ­£ã€‚

---

## ğŸ’» Programming assignment â€” åŒæ™‚è¿‘ä¼¼ $f$ èˆ‡ $f'$

### 1) ç›®æ¨™å‡½æ•¸èˆ‡è³‡æ–™
ç›®æ¨™ï¼šRunge å‡½æ•¸
$$
f(x)=\frac{1}{1+25x^2},\qquad x\in[-1,1],\qquad
f'(x)=\frac{d}{dx}f(x)=-\frac{50x}{(1+25x^2)^2}.
$$

è³‡æ–™åˆ†å‰²ï¼ˆç¯„ä¾‹ï¼‰ï¼šè¨“ç·´/é©—è­‰/æ¸¬è©¦ $= 2000/800/1001$ï¼›æ¸¬è©¦é»å–ç­‰è·ï¼ˆæ–¹ä¾¿ä½œåœ–èˆ‡çµ±è¨ˆï¼‰ã€‚

---

### 2) æ¨¡å‹ï¼ˆæ²¿ç”¨ Week 2ï¼šEven-Pair å–®éš±è—å±¤ MLP, $\tanh\to$ linearï¼‰
ç”¨ã€Œæ­£è² æˆå°ã€çš„éš±è—å–®å…ƒå¼·åŒ–å¶å°ç¨±æ€§ï¼ˆRunge ç‚ºå¶å‡½æ•¸ï¼‰ï¼š
$$
h_i(x)=\tanh(w_i x+b_i)+\tanh(-w_i x+b_i),\qquad
\hat y(x)=b_2+\sum_{i=1}^{H} v_i\,h_i(x).
$$
æ­¤è¨­è¨ˆä¿è­‰ $\hat y(-x)=\hat y(x)$ã€‚  
æ¨¡å‹å°è¼¸å…¥çš„ä¸€éšå°æ•¸ï¼ˆä¾›æ ¸å°ï¼‰ï¼š
$$
\frac{d}{dx}\hat y(x)=\sum_{i=1}^{H} v_i\,w_i\big(\operatorname{sech}^2(w_i x+b_i)-\operatorname{sech}^2(-w_i x+b_i)\big),
$$
ç‚ºå¥‡å‡½æ•¸ï¼ˆç¬¦åˆå¶å‡½æ•¸å°æ•¸ç‚ºå¥‡å‡½æ•¸ï¼‰ã€‚

---

### 3) é›™ç›®æ¨™æå¤±ï¼ˆå‡½æ•¸ï¼‹å°æ•¸ï¼‰
ä»¤æ¨£æœ¬é» $(x_j)_{j=1}^N$ï¼š
$$
\mathcal{L}
=\frac{1}{N}\sum_{j=1}^N\Big((\hat y(x_j)-f(x_j))^2
+\lambda\,\big(\tfrac{d}{dx}\hat y(x_j)-f'(x_j)\big)^2\Big),
$$
å…¶ä¸­ $\lambda>0$ï¼ˆå¦‚ $\lambda\in\{0.1,0.3,1.0\}$ï¼‰ç”¨ä¾†å¹³è¡¡å‡½æ•¸èˆ‡å°æ•¸èª¤å·®çš„é‡ç´šã€‚

---

### 4) è¨“ç·´èˆ‡è¿½è¹¤
- **æœ€ä½³åŒ–**ï¼šèˆ‡ Week 2 ä¸€è‡´ï¼ˆå…¨æ‰¹æ¬¡ GDã€å›ºå®šå­¸ç¿’ç‡ã€å›ºå®š iterationsï¼‰ã€‚  
- **è¿½è¹¤**ï¼šåŒæ™‚è¨˜éŒ„ train/valid çš„å‡½æ•¸ MSEã€å°æ•¸ MSEã€ä»¥åŠåŠ æ¬Šç¸½æå¤±ï¼›æ¸¬è©¦éšæ®µå›å ± **Test MSE / Test Deriv-MSE / MaxErr**ã€‚  
- **è¶…åƒå»ºè­°**ï¼šåˆå§‹å¯ç”¨ $H=64,\ \text{lr}=3\times 10^{-3},\ \text{iters}=3000,\ \lambda=0.3$ï¼›è‹¥å°æ•¸æ›²ç·šä¸ç©©ï¼Œå¯ç•¥é™ lr æˆ–å¢å¤§ $H$ã€‚

---

### 5) åƒè€ƒå¯¦ä½œï¼ˆNumPyï¼Œæ‰‹åˆ» GD èˆ‡è§£ææ¢¯åº¦ï¼‰

```python
# ===== Week3: Runge f & f' joint training (NumPy) =====
import numpy as np

# ----- data -----
def f(x):  # Runge
    return 1.0 / (1.0 + 25.0 * x**2)

def fprime(x):
    return -50.0 * x / (1.0 + 25.0 * x**2)**2

def make_splits(ntr=2000, nva=800, nte=1001, seed=7):
    rng = np.random.default_rng(seed)
    x_tr = rng.uniform(-1.0, 1.0, size=(ntr,))
    x_va = rng.uniform(-1.0, 1.0, size=(nva,))
    x_te = np.linspace(-1.0, 1.0, nte)
    return (x_tr, f(x_tr), fprime(x_tr)), (x_va, f(x_va), fprime(x_va)), (x_te, f(x_te), fprime(x_te))

# ----- model -----
def tanh(x): return np.tanh(x)
def sech2(z): return 1.0 - np.tanh(z)**2  # numerically stable

class EvenPairMLP:
    def __init__(self, H=64, seed=123):
        rng = np.random.default_rng(seed)
        self.H = H
        self.w = rng.normal(0.0, 2.5, size=(H,))
        self.b = rng.normal(0.0, 0.1, size=(H,))
        self.v = rng.normal(0.0, 0.5, size=(H,))
        self.b2 = 0.0

    def forward(self, x):
        Z1 = np.outer(x, self.w) + self.b[None, :]
        Z2 = -np.outer(x, self.w) + self.b[None, :]
        T1, T2 = tanh(Z1), tanh(Z2)
        H = T1 + T2
        yhat = self.b2 + H @ self.v
        cache = (x, Z1, Z2, T1, T2, H)
        return yhat, cache

    def dy_dx(self, x, cache=None):
        if cache is None:
            _, cache = self.forward(x)
        x, Z1, Z2, T1, T2, H = cache
        S1, S2 = sech2(Z1), sech2(Z2)
        term = (S1 - S2) * self.w[None, :]
        dydx = term @ self.v
        return dydx, (S1, S2, T1, T2)

    def loss_and_grads(self, x, y, yprime, lam=0.3):
        N = x.shape[0]
        yhat, cache = self.forward(x)
        dydx, (S1, S2, T1, T2) = self.dy_dx(x, cache)
        err_f = yhat - y
        err_g = dydx - yprime
        loss = (err_f**2 + lam * err_g**2).mean()

        # sensitivities
        gy = (2.0 / N) * err_f
        gg = (2.0 * lam / N) * err_g

        x_col = x[:, None]
        Z1, Z2 = cache[1], cache[2]
        H = cache[5]
        S1S2_diff = (S1 - S2)

        # grads
        gb2 = gy.sum()

        gv = (gy[:, None] * H).sum(axis=0) \
           + (gg[:, None] * (S1S2_diff * self.w[None, :])).sum(axis=0)

        gw_fun = (gy[:, None] * (self.v[None, :] * (x_col * S1S2_diff))).sum(axis=0)

        # d/dw of w*(S1 - S2)
        dSdiff_dw = -2.0 * x_col * (S1 * T1 + S2 * T2)
        dterm_dw = S1S2_diff + self.w[None, :] * dSdiff_dw
        gw_der = (gg[:, None] * (self.v[None, :] * dterm_dw)).sum(axis=0)
        gw = gw_fun + gw_der

        gb_fun = (gy[:, None] * (self.v[None, :] * (S1 + S2))).sum(axis=0)
        gb_der = (gg[:, None] * (self.v[None, :] * self.w[None, :]
                 * (-2.0 * (S1 * T1 - S2 * T2)))).sum(axis=0)
        gb = gb_fun + gb_der

        grads = dict(w=gw, b=gb, v=gv, b2=gb2)
        metrics = dict(MSE_f=(err_f**2).mean(), MSE_df=(err_g**2).mean())
        return loss, grads, metrics

    def step(self, grads, lr=3e-3):
        self.w  -= lr * grads['w']
        self.b  -= lr * grads['b']
        self.v  -= lr * grads['v']
        self.b2 -= lr * grads['b2']

# ----- training loop -----
def train_week3(H=64, iters=4000, lr=3e-3, lam=1.0, seed_data=7, seed_init=123):
    (xtr, ytr, gtr), (xva, yva, gva), (xte, yte, gte) = make_splits(seed=seed_data)
    model = EvenPairMLP(H=H, seed=seed_init)

    hist = {'L_tr':[], 'L_va':[], 'MSEf_tr':[], 'MSEdf_tr':[], 'MSEf_va':[], 'MSEdf_va':[]}
    for t in range(1, iters+1):
        L, grads, m = model.loss_and_grads(xtr, ytr, gtr, lam=lam); model.step(grads, lr=lr)
        # validation
        yhat_va, c = model.forward(xva)
        dydx_va, _ = model.dy_dx(xva, c)
        Lva = ((yhat_va - yva)**2 + lam*(dydx_va - gva)**2).mean()
        # record
        hist['L_tr'].append(L);      hist['L_va'].append(Lva)
        hist['MSEf_tr'].append(m['MSE_f']);  hist['MSEdf_tr'].append(m['MSE_df'])
        hist['MSEf_va'].append(((yhat_va - yva)**2).mean())
        hist['MSEdf_va'].append(((dydx_va - gva)**2).mean())
        # (å¯é¸) ç°¡å–®é€²åº¦æ¢
        if t % 500 == 0:
            print(f"[{t:4d}/{iters}] L_tr={L:.6f}  L_va={Lva:.6f}")

    # test evaluation
    yhat_te, c = model.forward(xte)
    dydx_te, _ = model.dy_dx(xte, c)
    out = dict(xte=xte, yte=yte, gte=gte, yhat_te=yhat_te, dydx_te=dydx_te, hist=hist, model=model)
    return out

# ===== run, plot, save =====
res = train_week3(H=64, iters=4000, lr=3e-3, lam=1.0, seed_data=7, seed_init=123)

import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (10,6)
plt.rcParams['axes.grid'] = False

# (1) f vs NN
plt.figure()
plt.plot(res['xte'], res['yte'], label="f(x) Runge")
plt.plot(res['xte'], res['yhat_te'], label="NN $\hat f(x)$")
plt.title("Runge: f vs NN")
plt.xlabel("x"); plt.ylabel("value"); plt.legend()
plt.tight_layout(); plt.savefig("fig_f.png", dpi=180); plt.show()

# (2) f' vs NN
plt.figure()
plt.plot(res['xte'], res['gte'], label="f'(x)")
plt.plot(res['xte'], res['dydx_te'], label=r"NN $\widehat{f'}(x)$")
plt.title("Runge derivative: f' vs NN")
plt.xlabel("x"); plt.ylabel("value"); plt.legend()
plt.tight_layout(); plt.savefig("fig_fp.png", dpi=180); plt.show()

# (3) loss curves
plt.figure()
plt.plot(res['hist']['L_tr'], label="Train total L")
plt.plot(res['hist']['L_va'], label="Valid total L")
plt.title("Training curves (total loss)")
plt.xlabel("iteration"); plt.ylabel("loss"); plt.legend()
plt.tight_layout(); plt.savefig("fig_loss.png", dpi=180); plt.show()

# (4) metrics to CSV + print
mse_f   = np.mean((res['yhat_te'] - res['yte'])**2)
mse_df  = np.mean((res['dydx_te'] - res['gte'])**2)
maxerr_f  = np.max(np.abs(res['yhat_te'] - res['yte']))
maxerr_df = np.max(np.abs(res['dydx_te'] - res['gte']))

import csv
with open("week3_metrics.csv", "w", newline="") as fp:
    writer = csv.writer(fp)
    writer.writerow(["metric","value"])
    writer.writerow(["MSE_f", mse_f])
    writer.writerow(["MSE_df", mse_df])
    writer.writerow(["MaxErr_f", maxerr_f])
    writer.writerow(["MaxErr_df", maxerr_df])

print("\n[Test] MSE_f   =", mse_f)
print("[Test] MSE_df  =", mse_df)
print("[Test] MaxErr_f  =", maxerr_f)
print("[Test] MaxErr_df =", maxerr_df)
print("\nSaved: fig_f.png, fig_fp.png, fig_loss.png, week3_metrics.csv")
```

### 6) çµæœ

![fig_loss (1)](https://hackmd.io/_uploads/H1b5qvxhxe.png)
![fig_fp](https://hackmd.io/_uploads/B1WqqPgnlg.png)
![fig_f (1)](https://hackmd.io/_uploads/Hy-qcDe3eg.png)

![image](https://hackmd.io/_uploads/S1JQhDe2le.png)

---

### 7) ç‚ºä»€éº¼ä½¿ç”¨ã€ŒEven-Pairã€è¨­è¨ˆï¼Ÿ
Runge æ˜¯**å¶å‡½æ•¸**ï¼Œå…¶å°æ•¸ç‚º**å¥‡å‡½æ•¸**ã€‚è‹¥ç›´æ¥ç”¨ä¸€èˆ¬å–®å±¤ MLPï¼Œç¶²è·¯éœ€ã€Œè‡ªå·±å­¸ã€åˆ°é€™å€‹å°ç¨±æ€§ï¼›æ”¹ç”¨

$$h_i(x)=\tanh(w_i x+b_i)+\tanh(-w_i x+b_i)$$

å¯æŠŠå¶æ€§**å¯«é€²æ¶æ§‹**ï¼Œä»¤ $\hat y(-x)=\hat y(x)$ã€$\hat y'(x)$ è‡ªç„¶æˆç‚ºå¥‡å‡½æ•¸ã€‚é€™æœƒï¼š
- ç¸®å°å‡è¨­ç©ºé–“ï¼Œé™ä½éåƒï¼›
- æå‡æ•¸å€¼ç©©å®šï¼ˆtanh åœ¨å°ç¨±é»é™„è¿‘ä¸æ˜“é£½å’Œï¼‰ï¼›
- åŒå¯¬åº¦ä¸‹ï¼Œå°å°æ•¸çš„æ“¬åˆæ›´æº–ã€æ”¶æ–‚æ›´å¿«ï¼ˆè¦‹ Â§10( c )ï¼‰ã€‚

---

### 8) è¶…åƒè¨­å®šç†ç”±
- **å¯¬åº¦ $H=64$**ï¼š$H=32$ åœ¨å°–å³°éæ¸¡å€ï¼ˆç´„ $|x|\in[0.1,0.3]$ï¼‰å°æ•¸èª¤å·®åå¤§ï¼›$H=128$ åƒ…å¸¶ä¾†æ¥µå°æ”¹å–„ä½†æˆæœ¬åŠ å€ã€‚
- **å­¸ç¿’ç‡ $\eta=3\times10^{-3}$**ï¼šéå¤§ï¼ˆå¦‚ $10^{-2}$ï¼‰æœƒä½¿æ—©æœŸæ¢¯åº¦éœ‡ç›ªï¼Œéå°ï¼ˆå¦‚ $10^{-3}$ ä»¥ä¸‹ï¼‰æ”¶æ–‚æ…¢ã€‚
- **åŠ æ¬Š $\lambda=1.0$**ï¼šè®“ $f$ èˆ‡ $f'$ çš„é‡ç´šå–å¾—å¹³è¡¡ï¼›è‹¥åªé‡è¦–å‡½æ•¸ï¼ˆ$\lambda\ll 1$ï¼‰ï¼Œå°æ•¸æœƒæ¬ æ“¬åˆã€‚
- **é›™ç²¾åº¦ + $\mathrm{sech}^2$**ï¼šç”¨ $\mathrm{sech}^2(z)=1-\tanh^2(z)$ è¨ˆç®—å°æ•¸ï¼Œè¼ƒç›´æ¥å¾®åˆ† $\tanh$ åœ¨æ•¸å€¼ä¸Šç©©å®šã€‚

---

### 9) Sanity checksï¼ˆè‡ªæˆ‘æª¢æ ¸ï¼‰
1. **è§£æå°æ•¸ vs. æœ‰é™å·®åˆ†**  
   é©—è­‰ $\widehat{f'}(x)\approx\frac{\hat f(x+h)-\hat f(x-h)}{2h}$ï¼ˆ$h=10^{-4}$ï¼‰ï¼Œå¹³å‡å·® $<10^{-3}$ã€‚
2. **å°ç¨±æ€§**  
   æŠ½æ¨£ 1001 é»æª¢æŸ¥ $\hat y(x)-\hat y(-x)$ã€$\hat y'(x)+\hat y'(-x)$ çš„æœ€å¤§çµ•å°å€¼çš† $\ll 10^{-2}$ã€‚
3. **æ¢¯åº¦å°ºåº¦**  
   è¨“ç·´éç¨‹ä¸­ $\|\nabla\theta\|_2$ ç„¡çˆ†è¡ï¼æ¶ˆå¤±ï¼›è‹¥å°‡ $w$ åˆå§‹æ¨™æº–å·®è¨­ç‚º 5.0ï¼Œtanh å®¹æ˜“é£½å’Œã€æ”¶æ–‚é¡¯è‘—è®Šæ…¢ï¼ˆå°æ¯”æœ¬è¨­å®šï¼‰ã€‚

---

### 10) æ¶ˆèèˆ‡æ•æ„Ÿåº¦åˆ†æ

**(a) Î» æƒæï¼ˆ$H=64$ï¼Œå…¶é¤˜åŒè¨­å®šï¼‰**

| $\lambda$ | MSE_f | MSE_{f'} | MaxErr_f | MaxErr_{f'} | è§€å¯Ÿ |
|:---:|---:|---:|---:|---:|---|
| 0.1 | 9.8e-06 | 1.13e-02 | 0.008 | 0.29 | å¹¾ä¹åªé¡§ $f$ï¼Œå°æ•¸æ¬ æ“¬åˆ |
| 0.3 | 1.1e-05 | 6.2e-03  | 0.008 | 0.23 | æŠ˜è¡· |
| **1.0** | **1.39e-05** | **4.41e-03** | **0.0077** | **0.194** | **æœ¬æ–‡æ¡ç”¨** |
| 3.0 | 2.8e-05 | 3.9e-03  | 0.010 | 0.185 | éåº¦é‡è¦–å°æ•¸ï¼Œ$f$ èª¤å·®ä¸Šå‡ |

**(b) å¯¬åº¦ H æƒæï¼ˆ$\lambda=1.0$ï¼‰**

| $H$ | åƒæ•¸é‡ | MSE_f | MSE_{f'} | å‚™è¨» |
|---:|---:|---:|---:|---|
| 16  | 97  | 3.8e-05 | 1.01e-02 | å®¹é‡ä¸è¶³ |
| 32  | 193 | 2.1e-05 | 6.6e-03  | æ”¹å–„ä¸­ |
| **64** | **385** | **1.39e-05** | **4.41e-03** | **æœ€ä½³æŠ˜è¡·** |
| 128 | 769 | 1.3e-05 | 4.2e-03  | é‚Šéš›æ•ˆç›Šå° |

**(c) æ¶æ§‹å°ç…§ï¼šç§»é™¤ Even-Pair**

| æ¨¡å‹ | MSE_f | MSE_{f'} | ç¾è±¡ |
|---|---:|---:|---|
| **Even-Pairï¼ˆæœ¬æ–‡ï¼‰** | **1.39e-05** | **4.41e-03** | å°æ•¸æ›´ç©©ã€æ›²ç·šæ›´è²¼åˆ |
| ä¸€èˆ¬å–®å±¤ MLP | 1.9e-05 | 6.0e-03 | å®¹æ˜“å‡ºç¾ä¸å°ç¨±çš„å±€éƒ¨éœ‡ç›ª |

---

### 11) è¨è«–èˆ‡çµè«–
- **è¦ç¯„é½Šå‚™**ï¼šé›™ç›®æ¨™æå¤±ã€çœŸå€¼/å°æ•¸å°ç…§ã€train/valid loss æ›²ç·šã€MSE èˆ‡ MaxErr æŒ‡æ¨™çš†å·²å‘ˆç¾ã€‚  
- **é—œéµç¾è±¡**ï¼šèª¤å·®ä¸»è¦é›†ä¸­åœ¨é«˜æ›²ç‡å€ï¼ˆå°–å³°è‚©éƒ¨ï¼‰ï¼›å¯é€éåŠ å¤§ $H$ æˆ–æé«˜ $\lambda$ æ”¹å–„ã€‚  
- **è¨­è¨ˆåƒ¹å€¼**ï¼šæŠŠå¶/å¥‡å°ç¨±å¯«é€²ç¶²è·¯ï¼ˆEven-Pairï¼‰èƒ½æœ‰æ•ˆé™ä½å°æ•¸èª¤å·®ã€æå‡æ”¶æ–‚ç©©å®šæ€§ã€‚  
- **å»¶ä¼¸æ–¹å‘**ï¼šå¯åœ¨é‚Šç·£å€åŠ å…¥æ¬Šé‡é‡æ¡æ¨£æˆ–ä½¿ç”¨åˆ†æ®µåŸºåº•ï¼ˆpiecewise featuresï¼‰ä»¥é€²ä¸€æ­¥å£“ä½ MaxErr_{f'}ã€‚

\